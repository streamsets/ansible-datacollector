#
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
#

# A unique identifier for the datacollector, if not set <hostname>:<http.port> is used.
# <hostname> is resolved using 'hostname -f'
{% if sdc_id is defined %}
sdc.id={{ sdc_id }}
{% else %}
#sdc.id=<hostname>:<port>
{% endif %}

# The base URL of the datacollector, used to create email alert messages.
# If not set http://<hostname>:<http.port> is used
# <hostname> is resolved using 'hostname -f'
{% if sdc_base_http_url is defined %}
sdc.base.http.url={{ sdc_base_http_url }}
{% else %}
#sdc.base.http.url=http://<hostname>:<port>
{% endif %}

# HTTP configuration
# The port the data collector runs the SDC HTTP endpoint.
# If different that -1, the SDC will run on this port
# If 0, the SDC will pick up a random port
# If the https.port is different that -1 or 0 and http.port is different than -1 or 0, the HTTP endpoint
# will redirect to the HTTPS endpoint.
http.port={{ http_port | default(18630) }}

# HTTPS configuration
# The port the data collector runs the SDC HTTPS endpoint.
# If different that -1, the SDC will run over SSL on this port
# If 0, the SDC will pick up a random port
https.port={{ https_port | default(-1) }}

# Java keystore file, in the SDC 'etc/' configuration directory
https.keystore.path={{ sdc_keystore_path | default("keystore.jks") }}

# Password for the keystore file,
# By default, the password is loaded from the 'sdc-keystore-password.txt'
# from the SDC 'etc/' configuration directory
https.keystore.password={{ sdc_keystore_password | default("@sdc-keystore-password.txt@") }}

# The authentication for the HTTP endpoint of the data collector
# Valid values are: 'none', 'digest' and 'form'
#
http.authentication={{ http_authentication | default("form") }}

# Check the permissions of the realm file should be owner only
http.realm.file.permission.check={{ http_realm_file_permission_check | default("true") }}

# Runs the data collector within a Kerberos session which is propagated to all stages.
# This is useful for stages that require Kerberos authentication with the services they interact with
kerberos.client.enabled={{ kerberos_client_enabled | default("false") }}

# The kerberos principal to use for the Kerberos session.
# It should be a service principal. If the hostname part of the service principal is '_HOST' or '0.0.0.0',
# the hostname will be replaced with the actual complete hostname of the data collector as advertised by the
# unix command 'hostname -f'
kerberos.client.principal={{ kerberos_client_principal | default("sdc/_HOST@EXAMPLE.COM") }}

# The location of the keytab file for the specified principal. If the path is relative, the keytab file will be
# looked under the data collector configuration directory
kerberos.client.keytab={{ kerberos_client_keytab | default("sdc.keytab") }}

# The realm used for authentication
# A file with the realm name and '.properties' extension must exist in the data collector configuration directory
# If this property is not set, the realm name is '<http.authentication>-realm'
{% if http_digest_realm is defined %}}
http.digest.realm={{ http_digest_realm }}
{% else %}
#http.digest.realm=local-realm
{% endif %}

preview.maxBatchSize={{ preview_max_batch_size | default(10) }}
preview.maxBatches={{ preview_max_batches | default(10) }}

production.maxBatchSize={{ production_max_batch_size | default(1000) }}

#This option determines the number of error records, per stage, that will be retained in memory when the pipeline is
#running. If set to zero, error records will not be retained in memory.
#If the specified limit is reached the oldest records will be discarded to make room for the newest one.
production.maxErrorRecordsPerStage={{ production_max_error_records_per_stage | default(100) }}

#This option determines the number of pipeline errors that will be retained in memory when the pipeline is
#running. If set to zero, pipeline errors will not be retained in memory.
#If the specified limit is reached the oldest error will be discarded to make room for the newest one.
production.maxPipelineErrors={{ production_max_pipeline_errors | default(100) }}

# Max number of concurrent REST calls allowed for the /rest/v1/admin/log endpoint
max.logtail.concurrent.requests={{ max_logtail_concurrent_requests | default(5) }}

# Max number of concurrent WebSocket calls allowed
max.webSockets.concurrent.requests={{ max_websockets_concurrent_requests | default(15) }}

ui.local.help.base.url=/docs
ui.hosted.help.base.url=http://streamsets.com/documentation/datacollector/{{ sdc_version }}/help/

ui.refresh.interval.ms={{ ui_refresh_interval_ms | default(2000) }}
ui.jvmMetrics.refresh.interval.ms={{ ui_jvm_metrics_refresh_interval | default(4000) }}

# SDC sends anonymous usage information using Google Analytics to StreamSets.
ui.enable.usage.data.collection={{ ui_enable_usage_data_collection | default("true") }}

# If true SDC UI will use WebSocket to fetch pipeline status/metrics/alerts otherwise UI will poll every few seconds
# to get the Pipeline status/metrics/alerts.
ui.enable.webSocket={{ ui_enable_websocket | default("true") }}

# Number of changes supported by undo/redo functionality.
# UI archives Pipeline Configuration/Rules in browser memory to support undo/redo functionality.
ui.undo.limit={{ ui_undo_limit | default(10) }}

# SMTP configuration to send alert emails
# All properties starting with 'mail.' are used to create the JavaMail session, supported protocols are 'smtp' & 'smtps'
mail.transport.protocol={{ mail_transport_protocol | default("smtp") }}
mail.smtp.host={{ mail_smtp_host | default("localhost") }}
mail.smtp.port={{ mail_smtp_port | default(25) }}
mail.smtp.auth={{ mail_smtp_auth | default("false") }}
mail.smtp.starttls.enable={{ mail_smtp_starttls_enable | default("false") }}
mail.smtps.host={{ mail_smtps_host | default("localhost") }}
mail.smtps.port={{ mail_smtps_port | default(465) }}
mail.smtps.auth={{ mail_smtps_auth | default("false") }}
# If 'mail.smtp.auth' or 'mail.smtps.auth' are to true, these properties are used for the user/password credentials,
# @email-password.txt@ will load the value from the 'email-password.txt' file in the config directory (where this file is)
xmail.username={{ xmail_username | default("foo") }}
xmail.password={{ xmail_password | default("@email-password.txt@") }}
# FROM email address to use for the messages
xmail.from.address={{ xmail_from_address | default("sdc@$localhost") }}

#Indicates the location where runtime configuration properties can be found.
#Value 'embedded' implies that the runtime configuration properties are present in this file and are prefixed with
#'runtime.conf_'.
#A value other than 'embedded' is treated as the name of a properties file from which the runtime configuration
#properties must be picked up. Note that the properties should not be prefixed with 'runtime.conf_' in this case.
runtime.conf.location={{ runtime_conf_location | default("embedded") }}

#Observer related

#The size of the queue where the pipeline queues up data rule evaluation requests.
#Each request is for a stream and contains sampled records for all rules that apply to that lane.
observer.queue.size={{ observer_queue_size | default(100) }}

#Sampled records which pass evaluation are cached for user to view. This determines the size of the cache and there is
#once cache per data rule
observer.sampled.records.cache.size={{ observer_sampled_records_cache_size | default(100) }}

#The time to wait before dropping a data rule evaluation request if the observer queue is full.
observer.queue.offer.max.wait.time.ms={{ observer_queue_offer_max_wait_time_ms | default(1000) }}


#Maximum number of private classloaders to allow in the data collector.
#Stage that have configuration singletons (i.e. Hadoop FS & Hbase) require private classloaders
max.stage.private.classloaders={{ max_stage_private_classloaders | default(50) }}


# Library aliases mapping to keep backward compatibility on pipelines when library names change
# The current aliasing mapping is to handle 1.0.0beta2 to 1.0.0 library names changes
#
# IMPORTANT: Under normal circumstances all these properties should not be changed
#
library.alias.streamsets-datacollector-apache-kafka_0_8_1_1-lib=streamsets-datacollector-apache-kafka_0_8_1-lib
library.alias.streamsets-datacollector-apache-kafka_0_8_2_0-lib=streamsets-datacollector-apache-kafka_0_8_2-lib
library.alias.streamsets-datacollector-apache-kafka_0_8_2_1-lib=streamsets-datacollector-apache-kafka_0_8_2-lib
library.alias.streamsets-datacollector-cassandra_2_1_5-lib=streamsets-datacollector-cassandra_2-lib
library.alias.streamsets-datacollector-cdh5_2_1-lib=streamsets-datacollector-cdh_5_2-lib
library.alias.streamsets-datacollector-cdh5_2_3-lib=streamsets-datacollector-cdh_5_2-lib
library.alias.streamsets-datacollector-cdh5_2_4-lib=streamsets-datacollector-cdh_5_2-lib
library.alias.streamsets-datacollector-cdh5_3_0-lib=streamsets-datacollector-cdh_5_3-lib
library.alias.streamsets-datacollector-cdh5_3_1-lib=streamsets-datacollector-cdh_5_3-lib
library.alias.streamsets-datacollector-cdh5_3_2-lib=streamsets-datacollector-cdh_5_3-lib
library.alias.streamsets-datacollector-cdh5_4_0-cluster-cdh_kafka_1_2_0-lib=streamsets-datacollector-cdh_5_4-cluster-cdh_kafka_1_2_0-lib
library.alias.streamsets-datacollector-cdh5_4_0-lib=streamsets-datacollector-cdh_5_4-lib
library.alias.streamsets-datacollector-cdh5_4_1-cluster-cdh_kafka_1_2_0-lib=streamsets-datacollector-cdh_5_4-cluster-cdh_kafka_1_2_0-lib
library.alias.streamsets-datacollector-cdh5_4_1-lib=streamsets-datacollector-cdh_5_4-lib
library.alias.streamsets-datacollector-cdh_kafka_1_2_0-lib=streamsets-datacollector-cdh_kafka_1_2_0-lib
library.alias.streamsets-datacollector-elasticsearch_1_4_4-lib=streamsets-datacollector-elasticsearch_1_4-lib
library.alias.streamsets-datacollector-elasticsearch_1_5_0-lib=streamsets-datacollector-elasticsearch_1_5-lib
library.alias.streamsets-datacollector-hdp_2_2_0-lib=streamsets-datacollector-hdp_2_2-lib
library.alias.streamsets-datacollector-jython_2_7_0-lib=streamsets-datacollector-jython_2_7-lib
library.alias.streamsets-datacollector-mongodb_3_0_2-lib=streamsets-datacollector-mongodb_3-lib

# Stage aliases for mapping to keep backward compatibility on pipelines when stages move libraries
# The current alias mapping is to handle moving the jdbc stages to their own library
#
# IMPORTANT: Under normal circumstances all these properties should not be changed
#
stage.alias.streamsets-datacollector-basic-lib,com_streamsets_pipeline_stage_destination_jdbc_JdbcDTarget=streamsets-datacollector-jdbc-lib,com_streamsets_pipeline_stage_destination_jdbc_JdbcDTarget
stage.alias.streamsets-datacollector-basic-lib,com_streamsets_pipeline_stage_origin_jdbc_JdbcDSource=streamsets-datacollector-jdbc-lib,com_streamsets_pipeline_stage_origin_jdbc_JdbcDSource
